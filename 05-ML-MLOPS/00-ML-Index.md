# ML & MLOps Index

ML interviews vary wildly by company and role. The consistent theme: can you build reliable ML systems, not just train models.

## Core pillars

1. **Modeling fundamentals** (bias/variance, metrics, evaluation)
2. **Data** (quality, leakage, features, labeling)
3. **Production** (deployment, monitoring, iteration)

## Production mindset (high signal)

Two reference frames this repo uses:

- Rules of ML (engineering-first approach)
- ML Test Score (tests/monitoring rubric for production readiness)

See [RESOURCES.md](../RESOURCES.md).

## What changes in 2025â€“2026

- More interviews explicitly evaluate how you work with AI tools.
- LLM/GenAI systems design shows up more (retrieval, evaluation, safety, cost).
- MLOps signals matter more: monitoring, rollback, shadow deploy, data issues.

## Next pages

- [05-ML-MLOPS/01-ML-Fundamentals-for-Interviews.md](01-ML-Fundamentals-for-Interviews.md)
- [05-ML-MLOPS/04-ML-Testing-and-ML-Test-Score.md](04-ML-Testing-and-ML-Test-Score.md)
- [05-ML-MLOPS/06-Monitoring-and-Drift.md](06-Monitoring-and-Drift.md)
